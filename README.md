# GPT-2
An implementation of GPT-2 inspired by the paper "Attention Is All You Need" and [Andrey Karpathy’s video](https://www.youtube.com/watch?v=l8pRSuU81PU).

What better way to learn about large language models (LLMs) than by building one from scratch?  
This project is my personal implementation of **GPT-2**, a landmark model that helped shape the modern era of generative AI.  

GPT-2, released by OpenAI in 2019, was one of the first large-scale transformer models capable of generating coherent, contextually relevant text. It popularized the transformer architecture introduced in the paper *"Attention Is All You Need"* and demonstrated the power of scaling language models.  

By re-implementing GPT-2, I aim to gain a deeper understanding of:  
- The transformer architecture (attention, feed-forward layers, residual connections, etc.)  
- Tokenization and text preprocessing  
- Training dynamics and optimization  
- How design choices in GPT-2 laid the groundwork for today’s models, including GPT-5.  

This repo serves both as a learning project and as a reference for anyone curious about how transformers work under the hood.  
